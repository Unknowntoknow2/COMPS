name: Scrape EchoPark SRP (robots-allowed, fixed)

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'EchoPark SRP URL under /used-cars/*'
        required: true
        default: 'https://www.echopark.com/used-cars/'

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install scrapy==2.11.2 scrapy-playwright==0.0.32

      - name: Install Playwright Chromium + deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare workspace
        run: |
          mkdir -p spider out logs

      - name: Save robots.txt (with UA)
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          set -e
          host="$(python - <<'PY'
          from urllib.parse import urlparse; import os
          print(urlparse(os.environ["START_URL"]).netloc)
          PY
          )"
          url="https://${host}/robots.txt"
          UA="AINCOMP/1.0 (+compliance)"
          if curl -A "$UA" -fsSL "$url" -o out/robots.txt; then
            echo "robots.txt saved from $url"
          else
            echo "FETCH_FAILED" > out/robots.txt
          fi

      - name: Write SRP spider (fixed)
        run: |
          cat > spider/echopark_srp.py <<'PY'
          import re, scrapy
          from urllib.parse import urlparse, urljoin
          from scrapy_playwright.page import PageMethod

          def _num(text):
              if not text: return None
              m = re.search(r'[\$£€]?\s*([\d,]+(?:\.\d+)?)', text)
              return float(m.group(1).replace(',', '')) if m else None

          def _miles(text):
              if not text: return None
              m = re.search(r'([\d,]+)\s*(?:mi|mile|miles)\b', text.lower())
              return int(m.group(1).replace(',', '')) if m else None

          def _year(text):
              if not text: return None
              m = re.search(r'\b(19|20)\d{2}\b', text)
              return int(m.group(0)) if m else None

          class EchoParkSRP(scrapy.Spider):
              name = "echopark_srp"
              custom_settings = {
                  "ROBOTSTXT_OBEY": True,
                  "USER_AGENT": "AINCOMP/1.0 (+compliance)",
                  "LOG_LEVEL": "INFO",
                  "DOWNLOAD_DELAY": 0.5,
                  "CONCURRENT_REQUESTS_PER_DOMAIN": 2,

                  # Playwright wiring
                  "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
                  "DOWNLOAD_HANDLERS": {
                      "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                      "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                  },
                  "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT": 30000,
                  "PLAYWRIGHT_LAUNCH_OPTIONS": { "headless": True },
                  "PLAYWRIGHT_PROCESS_REQUEST_HEADERS": lambda headers: {**headers, "user-agent":"AINCOMP/1.0 (+compliance)"},

                  # Output
                  "FEEDS": { "out/out.ndjson": { "format": "jsonlines", "overwrite": True } },
              }

              def __init__(self, url=None, *a, **kw):
                  super().__init__(*a, **kw)
                  if not url:
                      raise ValueError("Provide -a url=https://www.echopark.com/used-cars/... (robots-allowed)")
                  if "/used-cars/" not in url:
                      raise ValueError("URL must be under /used-cars/* per robots.txt")
                  self.start_url = url

              def start_requests(self):
                  yield scrapy.Request(
                      self.start_url,
                      meta={
                          "playwright": True,
                          "playwright_page_methods": [
                              PageMethod("wait_for_load_state", "networkidle"),
                              PageMethod("wait_for_selector", '[href*="/car/"]', state="attached", timeout=30000),
                              # Uncomment to capture a screenshot for debugging:
                              # PageMethod("screenshot", path="out/page.png", full_page=True),
                          ],
                      },
                      callback=self.parse_srp
                  )

              def parse_srp(self, response):
                  items = []

                  # Broad card grouping; then parse text with regex (no :contains)
                  cards = response.css('[data-test*="vehicle"], [class*="card"], article, li')
                  for c in cards:
                      link = c.css('a[href*="/car/"]::attr(href)').get()
                      if not link:  # treat only nodes that look like a vehicle card
                          continue
                      vdp_url = urljoin(response.url, link)

                      # gather all visible text inside the card
                      texts = c.css('*::text, ::text').getall()
                      full = " ".join(t.strip() for t in texts if t and t.strip())

                      title = c.css('h1::text, h2::text, h3::text, .title::text, [data-test*="title"]::text').get()
                      yr = _year(title) or _year(full)
                      price = _num(full)
                      miles = _miles(full)

                      itm = {
                          "title": title or None,
                          "year": yr,
                          "price": price,
                          "mileage": miles,
                          "vdp_url": vdp_url,  # NOTE: we do not request VDPs (/car/* disallowed)
                          "dealer_id": urlparse(response.url).netloc,
                          "_provenance": {
                              "source": response.url,
                              "robots_obeyed": True,
                              "note": "SRP only; VDPs /car/* blocked by robots."
                          }
                      }
                      if any(itm.get(k) for k in ("title","price","mileage","year")):
                          items.append(itm)

                  for it in items:
                      yield it

                  # Pagination within /used-cars/*
                  next_href = response.css('a[rel="next"]::attr(href), a:has-text("Next")::attr(href)').get() or None
                  if next_href:
                      next_url = urljoin(response.url, next_href)
                      if "/used-cars/" in urlparse(next_url).path:
                          yield scrapy.Request(
                              next_url,
                              meta={
                                  "playwright": True,
                                  "playwright_page_methods": [
                                      PageMethod("wait_for_load_state", "networkidle"),
                                      PageMethod("wait_for_selector", '[href*="/car/"]', state="attached", timeout=30000),
                                  ],
                              },
                              callback=self.parse_srp
                          )
          PY

      - name: Run spider
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          set -o pipefail
          python -m scrapy runspider spider/echopark_srp.py -a url="$START_URL" 2>&1 | tee logs/scrapy.log || true
          test -s out/out.ndjson || echo '{}' > out/out.ndjson

      - name: Upload artifacts (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: echopark-srp-output
          path: |
            out/out.ndjson
            out/robots.txt
            logs/scrapy.log
            out/page.png
