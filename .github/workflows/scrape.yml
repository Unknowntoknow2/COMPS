name: Run Craigslist Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: "0 */6 * * *"  # every 6 hours

jobs:
  scrape:
    runs-on: ubuntu-latest

    env:
      PROXY_POOL: ${{ secrets.PROXY_POOL }}
      POSTGRES_DSN: ${{ secrets.POSTGRES_DSN }}
      S3_BUCKET: vehicle_photos
      S3_ACCESS_KEY: ${{ secrets.S3_ACCESS_KEY }}
      S3_SECRET_KEY: ${{ secrets.S3_SECRET_KEY }}
      S3_ENDPOINT_URL: https://xltxqqzattxogxtqrggt.supabase.co/storage/v1/s3
      ANON_HMAC_SECRET: ${{ secrets.ANON_HMAC_SECRET }}
      LOG_LEVEL: INFO

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          echo "Installing dependencies..."
          pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install chromium --with-deps

      - name: Run Scraper and Upload to Supabase
        run: |
          echo "Starting Craigslist scrape job..."
          mkdir -p data logs
          scrapy crawl craigslist_cars \
            -a city=sfbay \
            -a query="honda civic" \
            -a limit=10 \
            -s LOG_FILE=logs/scrapy.log \
            -s LOG_LEVEL=INFO || true
          echo "Scraping complete. Checking output..."

          # Ensure file exists even if scraper fails
          if [ ! -f data/output.jsonl ]; then
            echo "No output file found. Creating placeholder..."
            echo "{}" > data/output.jsonl
          fi

      - name: Upload results and logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: craigslist-scraper-results
          path: |
            data/output.jsonl
            logs/scrapy.log
