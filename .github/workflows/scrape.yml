name: Scrape EchoPark SRP (robots-allowed)

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'EchoPark SRP URL under /used-cars/*'
        required: true
        default: 'https://www.echopark.com/used-cars/'

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install scrapy==2.11.2 scrapy-playwright==0.0.32

      - name: Install Playwright Chromium + deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare workspace
        run: |
          mkdir -p spider out logs

      - name: Save robots.txt (for transparency)
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          python - <<'PY'
          import os, sys
          from urllib.parse import urlparse
          import urllib.request
          url=os.environ["START_URL"]
          host=urlparse(url).netloc
          robots=f"https://{host}/robots.txt"
          try:
              with urllib.request.urlopen(robots, timeout=10) as r:
                  b=r.read()
              open("out/robots.txt","wb").write(b)
              print("robots.txt saved:", robots)
          except Exception as e:
              print("robots.txt fetch failed:", e, file=sys.stderr)
              open("out/robots.txt","w").write("FETCH_FAILED")
          PY

      - name: Write SRP spider (Scrapy + Playwright)
        run: |
          cat > spider/echopark_srp.py <<'PY'
          import re, json
          import scrapy
          from urllib.parse import urlparse, urljoin

          def _num(s):
              if s is None: return None
              m=re.search(r'[\d,]+(?:\.\d+)?', str(s))
              return float(m.group(0).replace(',','')) if m else None

          def _int(s):
              if s is None: return None
              m=re.search(r'[\d,]+', str(s))
              return int(m.group(0).replace(',','')) if m else None

          class EchoParkSRP(scrapy.Spider):
              name = "echopark_srp"
              custom_settings = {
                  "ROBOTSTXT_OBEY": True,
                  "USER_AGENT": "AINCOMP/1.0 (+compliance)",
                  "LOG_LEVEL": "INFO",
                  "DOWNLOAD_DELAY": 0.5,
                  "CONCURRENT_REQUESTS_PER_DOMAIN": 2,

                  # Playwright wiring
                  "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
                  "DOWNLOAD_HANDLERS": {
                      "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                      "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                  },
                  "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT": 15000,
                  "PLAYWRIGHT_LAUNCH_OPTIONS": { "headless": True },

                  # Output
                  "FEEDS": { "out/out.ndjson": { "format": "jsonlines", "overwrite": True } },
              }

              def __init__(self, url=None, *a, **kw):
                  super().__init__(*a, **kw)
                  if not url:
                      raise ValueError("Provide -a url=https://www.echopark.com/used-cars/... (robots-allowed)")
                  if "/used-cars/" not in url:
                      raise ValueError("URL must be under /used-cars/* per robots.txt")
                  self.start_url = url

              def start_requests(self):
                  yield scrapy.Request(
                      self.start_url,
                      meta={
                          "playwright": True,
                          "playwright_page_methods": [
                              ("wait_for_selector",
                               '[data-test*="vehicle"], [class*="card"], a[href*="/car/"]',
                               {"state":"attached", "timeout": 15000})
                          ],
                      },
                      callback=self.parse_srp
                  )

              def parse_srp(self, response):
                  items = []
                  # SRP vehicle cards (EchoPark is React; we use broad selectors)
                  cards = response.css(
                      '[data-test*="vehicle"], [class*="card"], article, li:has(a[href*="/car/"])'
                  )

                  for c in cards:
                      title = c.css('h2::text, h3::text, .title::text, [data-test*="title"]::text').get()
                      price_t = c.css('[class*="price"]::text, [data-test*="price"]::text, [aria-label*="Price"]::text').get()
                      mileage_t = c.css('[class*="mile"]::text, [data-test*="mile"]::text, li:contains("mile")::text').get()
                      href = c.css('a[href*="/car/"]::attr(href)').get()
                      vdp_url = urljoin(response.url, href) if href else None  # NOTE: do NOT fetch VDPs (/car/* disallowed)

                      year = None
                      if title:
                          m = re.search(r'\b(19|20)\d{2}\b', title)
                          if m: year = int(m.group(0))

                      itm = {
                          "title": title,
                          "year": year,
                          "price": _num(price_t),
                          "mileage": _int(mileage_t),
                          "vdp_url": vdp_url,
                          "dealer_id": urlparse(response.url).netloc,
                          "_provenance": {
                              "source": response.url,
                              "robots_obeyed": True,
                              "note": "VDPs under /car/* are disallowed and not fetched."
                          }
                      }
                      if any(itm.get(k) for k in ("title","price","mileage","vdp_url","year")):
                          items.append(itm)

                  for it in items:
                      yield it

                  # Pagination inside /used-cars/*
                  next_href = response.css('a[rel="next"]::attr(href), a:contains("Next")::attr(href)').get()
                  if next_href:
                      next_url = urljoin(response.url, next_href)
                      if "/used-cars/" in urlparse(next_url).path:
                          yield scrapy.Request(
                              next_url,
                              meta={
                                  "playwright": True,
                                  "playwright_page_methods": [
                                      ("wait_for_selector",
                                       '[data-test*="vehicle"], [class*="card"], a[href*="/car/"]',
                                       {"state":"attached", "timeout": 15000})
                                  ],
                              },
                              callback=self.parse_srp
                          )
          PY

      - name: Run spider
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          set -o pipefail
          python -m scrapy runspider spider/echopark_srp.py -a url="$START_URL" 2>&1 | tee logs/scrapy.log || true
          test -s out/out.ndjson || echo '{}' > out/out.ndjson

      - name: Upload artifacts (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: echopark-srp-output
          path: |
            out/out.ndjson
            out/robots.txt
            logs/scrapy.log
