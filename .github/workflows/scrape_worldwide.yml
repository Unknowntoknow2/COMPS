name: Scrape Local Dealer via Sitemap (robots-allowed)

on:
  workflow_dispatch:
    inputs:
      start_url:
        description: 'Dealer homepage (same domain as sitemap)'
        required: true
        default: 'https://www.worldwideautosalesca.com/'

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install scrapy==2.11.2 scrapy-playwright==0.0.32 lxml==5.3.0

      - name: Install Playwright Chromium
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare workspace
        run: |
          mkdir -p spider out logs

      - name: Save robots.txt (with UA)
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          set -e
          host="$(python - <<'PY'
          from urllib.parse import urlparse; import os
          print(urlparse(os.environ["START_URL"]).netloc)
          PY
          )"
          url="https://${host}/robots.txt"
          UA="AINCOMP/1.0 (+compliance)"
          if curl -A "$UA" -fsSL "$url" -o out/robots.txt; then
            echo "robots.txt saved from $url"
          else
            echo "FETCH_FAILED" > out/robots.txt
          fi

      - name: Write spider (sitemap→SRP→extract)
        run: |
          cat > spider/dealer_srp.py <<'PY'
          import re, scrapy, io
          from urllib.parse import urlparse, urljoin
          from scrapy_playwright.page import PageMethod
          from lxml import etree

          SRP_HINTS = ("inventory","used","pre-owned","preowned","search","vehicles","browse","listings","stock")
          VDP_HINTS = ("/vehicle", "vehicle-details", "viewdetails", "vdp", "vin=")

          def _num(s):
              if not s: return None
              m=re.search(r'[\$£€]?\s*([\d,]+(?:\.\d+)?)', s)
              return float(m.group(1).replace(',','')) if m else None

          def _miles(s):
              if not s: return None
              m=re.search(r'([\d,]+)\s*(?:mi|mile|miles)\b', s.lower())
              return int(m.group(1).replace(',','')) if m else None

          def _year(s):
              if not s: return None
              m=re.search(r'\b(19|20)\d{2}\b', s)
              return int(m.group(0)) if m else None

          def _vin(s):
              if not s: return None
              m=re.search(r'\b[A-HJ-NPR-Z0-9]{17}\b', s)
              return m.group(0) if m else None

          class DealerSRP(scrapy.Spider):
              name = "dealer_srp"
              custom_settings = {
                  "ROBOTSTXT_OBEY": True,
                  "USER_AGENT": "AINCOMP/1.0 (+compliance)",
                  "LOG_LEVEL": "INFO",
                  "DOWNLOAD_DELAY": 0.5,
                  "CONCURRENT_REQUESTS_PER_DOMAIN": 2,
                  "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
                  "DOWNLOAD_HANDLERS": {
                      "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                      "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                  },
                  "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT": 30000,
                  "PLAYWRIGHT_LAUNCH_OPTIONS": { "headless": True },
                  "FEEDS": { "out/out.ndjson": { "format": "jsonlines", "overwrite": True } },
              }

              def __init__(self, url=None, *a, **kw):
                  super().__init__(*a, **kw)
                  if not url: raise ValueError("Provide -a url=https://dealer/")
                  self.start_url = url
                  self.host = urlparse(url).netloc

              def start_requests(self):
                  # 1) Load homepage for screenshot (optional)
                  yield scrapy.Request(
                      self.start_url,
                      meta={
                          "playwright": True,
                          "playwright_page_methods": [
                              PageMethod("wait_for_load_state", "networkidle"),
                              PageMethod("screenshot", path="out/home.png", full_page=True),
                          ],
                      },
                      callback=self.after_home,
                  )

              def after_home(self, response):
                  # 2) Fetch sitemap.xml → pick SRP-like URLs
                  sm_url = urljoin(response.url, "/sitemap.xml")
                  yield scrapy.Request(sm_url, callback=self.parse_sitemap, dont_filter=True)

              def parse_sitemap(self, response):
                  urls=[]
                  try:
                      tree=etree.parse(io.BytesIO(response.body))
                      for loc in tree.xpath("//*[local-name()='loc']/text()"):
                          p=urlparse(loc)
                          if p.netloc != self.host: continue
                          path=p.path.lower()
                          if any(h in path for h in SRP_HINTS) and not any(v in path for v in VDP_HINTS):
                              urls.append(loc)
                  except Exception:
                      # fallback: regex find URLs in body
                      body=response.text
                      for m in re.finditer(r'<loc>(.*?)</loc>', body, re.I|re.M):
                          loc=m.group(1).strip()
                          p=urlparse(loc)
                          if p.netloc != self.host: continue
                          path=p.path.lower()
                          if any(h in path for h in SRP_HINTS) and not any(v in path for v in VDP_HINTS):
                              urls.append(loc)

                  # Dedup and cap to 5 SRPs
                  seen=set()
                  srps=[u for u in urls if not (u in seen or seen.add(u))][:5]
                  if not srps:
                      # If none in sitemap, try common SRP paths
                      guesses = [
                        "/inventory", "/used-vehicles", "/used", "/pre-owned", "/search", "/vehicles", "/inventory/used"
                      ]
                      base=f"https://{self.host}"
                      srps=[urljoin(base,g) for g in guesses]

                  for i,u in enumerate(srps):
                      yield scrapy.Request(
                          u,
                          meta={
                              "playwright": True,
                              "playwright_page_methods": [
                                  PageMethod("wait_for_load_state", "networkidle"),
                                  PageMethod("wait_for_selector", "a, [class*='vehicle'], [class*='inventory']", state="attached", timeout=30000),
                                  PageMethod("screenshot", path=f"out/srp_{i}.png", full_page=True),
                              ],
                          },
                          callback=self.parse_srp
                      )

              def parse_srp(self, response):
                  cards = response.css("[class*='vehicle'], [class*='inventory'], article, li, .card, .vehicle, .listing")
                  for c in cards:
                      href = c.css("a::attr(href)").get() or ""
                      if any(v in href.lower() for v in VDP_HINTS):
                          vdp_url=urljoin(response.url, href)
                      else:
                          # keep only likely VDP links
                          continue

                      texts = c.css("*::text, ::text").getall()
                      full = " ".join(t.strip() for t in texts if t and t.strip())
                      if not full: continue

                      title = c.css("h1::text, h2::text, h3::text, .title::text, .vehicle-title::text").get()
                      yr = _year(title) or _year(full)
                      price = _num(full)
                      miles = _miles(full)
                      vin = _vin(full) or _vin(href)

                      yield {
                          "title": title or None,
                          "year": yr,
                          "price": price,
                          "mileage": miles,
                          "vin": vin,
                          "vdp_url": vdp_url,  # not fetched; recorded for humans
                          "dealer_id": urlparse(response.url).netloc,
                          "_provenance": {
                              "source": response.url,
                              "robots_obeyed": True,
                              "note": "SRP only; VDPs not fetched."
                          }
                      }

                  # Pagination: look for next links within same domain
                  next_href = response.css('a[rel="next"]::attr(href), a.next::attr(href)').get()
                  if next_href:
                      nxt=urljoin(response.url, next_href)
                      if urlparse(nxt).netloc == self.host:
                          yield scrapy.Request(
                              nxt,
                              meta={
                                  "playwright": True,
                                  "playwright_page_methods": [
                                      PageMethod("wait_for_load_state", "networkidle"),
                                      PageMethod("wait_for_selector", "a, [class*='vehicle'], [class*='inventory']", state="attached", timeout=30000),
                                  ],
                              },
                              callback=self.parse_srp
                          )
          PY

      - name: Run spider
        env:
          START_URL: ${{ github.event.inputs.start_url }}
        run: |
          set -o pipefail
          python - <<'PY' > spider/run.py
          import os, sys, subprocess
          url=os.environ["START_URL"]
          cmd=["python","-m","scrapy","runspider","spider/dealer_srp.py","-a",f"url={url}"]
          p=subprocess.run(cmd, capture_output=True, text=True)
          open("logs/scrapy.log","w").write(p.stdout + "\n--- STDERR ---\n" + p.stderr)
          sys.exit(0)
          PY
          python spider/run.py
          test -s out/out.ndjson || echo '{}' > out/out.ndjson

      - name: Upload artifacts (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dealer-srp-output
          path: |
            out/out.ndjson
            out/robots.txt
            out/home.png
            out/srp_*.png
            logs/scrapy.log
